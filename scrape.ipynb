{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: pandas in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: selenium in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (4.30.0)\n",
      "Requirement already satisfied: webdriver-manager in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from beautifulsoup4) (4.13.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: trio~=0.17 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: exceptiongroup in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\menem\\web_scrapping\\.venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 pandas selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Myntra - Clothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping product: 'NoneType' object has no attribute 'text'\n",
      "Error scraping product: 'NoneType' object has no attribute 'text'\n",
      "Error scraping product: 'NoneType' object has no attribute 'text'\n",
      "Error scraping product: 'NoneType' object has no attribute 'text'\n",
      "Error scraping product: 'NoneType' object has no attribute 'text'\n",
      "Error scraping product: 'NoneType' object has no attribute 'text'\n",
      "Error scraping product: 'NoneType' object has no attribute 'text'\n",
      "Error scraping product: 'NoneType' object has no attribute 'text'\n",
      "                 brand                           name     price original_price\n",
      "0       THE BEAR HOUSE  Men Printed Oversized T-shirt  Rs. 1599       Rs. 1599\n",
      "1  StyleCast x Revolte          Men Relaxed Fit Jeans  Rs. 1631       Rs. 1631\n",
      "2               ONEWAY    Unisex Solid Cotton T-shirt   Rs. 559        Rs. 559\n",
      "3       THE BEAR HOUSE         Printed Cotton T-shirt  Rs. 1399       Rs. 1399\n",
      "4             Roadster    Men Light Fade Skater Jeans   Rs. 839        Rs. 839\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "def scrape_myntra(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3) \n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    products = []\n",
    "    \n",
    "    for item in soup.select('li.product-base'):\n",
    "        try:\n",
    "            brand_element = item.select_one('h3.product-brand').text\n",
    "            name_element = item.select_one('h4.product-product').text\n",
    "            price_element = item.select_one('span.product-discountedPrice').text\n",
    "            ori_price_element = item.select_one('span.product-discountedPrice').text\n",
    "            \n",
    "            product = {\n",
    "                'brand': brand_element,\n",
    "                'name': name_element,\n",
    "                'price': price_element,\n",
    "                'original_price': ori_price_element\n",
    "            }\n",
    "            products.append(product)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping product: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "url = \"https://www.myntra.com/men-clothing\"\n",
    "df = scrape_myntra(url)\n",
    "print(df.head())\n",
    "df.to_csv(\"myntra_scraped_products.csv\", index=False)  \n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meesho - Clothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     name current_price original_price rating  \\\n",
      "0            Urbane Ravishing Men Tshirts           300            300    3.8   \n",
      "1                 Trendy Retro Men Shirts           172            172    4.1   \n",
      "2             Trendy Glamorous Men Shirts           216            216    3.9   \n",
      "3          Classic Sensational Men Shirts           190            190    4.1   \n",
      "4                Classic Retro Men Shirts           228            228    3.9   \n",
      "5           Classy Fashionista Men Shirts           325            325     NA   \n",
      "6                        STI Men's Shirts           213            213    4.0   \n",
      "7           Classy Fashionista Men Shirts           242            242    4.5   \n",
      "8              Trendy Fabulous Men Shirts           295            295    3.8   \n",
      "9           Trendy Fashionista Men Shirts           459            459     NA   \n",
      "10           Fancy Fashionista Men Shorts           347            347    4.0   \n",
      "11           Stylish Glamorous Men Shirts           167            167    3.8   \n",
      "12   Dollar Bigboss Men's Innerwear Vests           148            148    4.3   \n",
      "13           Fancy Unique Men Track Pants           329            329    4.0   \n",
      "14  Dollar Lehar | Force NXT Men's Trunks           430            430    4.1   \n",
      "15          Trendy Fashionable Men Shirts           260            260    3.9   \n",
      "16          Men Polyester Regular Tshirts            92             92    3.8   \n",
      "17                    Gespo Men's Tshirts           241            241    3.9   \n",
      "18          Men Polyester Regular Tshirts           276            276     NA   \n",
      "19                       STI Men's Shirts           205            205    4.0   \n",
      "\n",
      "          reviews  \n",
      "0    1794 Reviews  \n",
      "1   19091 Reviews  \n",
      "2   27233 Reviews  \n",
      "3   25166 Reviews  \n",
      "4     504 Reviews  \n",
      "5       0 Reviews  \n",
      "6   63996 Reviews  \n",
      "7      51 Reviews  \n",
      "8    9843 Reviews  \n",
      "9       0 Reviews  \n",
      "10   9725 Reviews  \n",
      "11  59157 Reviews  \n",
      "12   2201 Reviews  \n",
      "13  15838 Reviews  \n",
      "14  10054 Reviews  \n",
      "15   5744 Reviews  \n",
      "16   1418 Reviews  \n",
      "17   5259 Reviews  \n",
      "18      0 Reviews  \n",
      "19  11263 Reviews  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "def scrape_meesho(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    products = []\n",
    "    \n",
    "    for item in soup.select('div.sc-dkrFOg.ProductListItem__GridCol-sc-1baba2g-0'):\n",
    "        try:\n",
    "            name = item.select_one('p.sc-eDvSVe.gQDOBc').text.strip()\n",
    "            price = item.select_one('h5.sc-eDvSVe.dwCrSh').text.strip()\n",
    "            original_price = item.select_one('s.sc-eDvSVe').text.strip() if item.select_one('s.sc-eDvSVe') else price\n",
    "            rating = item.select_one('span.sc-eDvSVe.laVOtN').text.strip() if item.select_one('span.sc-eDvSVe.laVOtN') else 'NA'\n",
    "            reviews = item.select_one('span.sc-eDvSVe.iaGtYc').text.strip() if item.select_one('span.sc-eDvSVe.iaGtYc') else '0 Reviews'\n",
    "            \n",
    "            product = {\n",
    "                'name': name,\n",
    "                'current_price': price.replace('₹', '').strip(),\n",
    "                'original_price': original_price.replace('₹', '').strip(),\n",
    "                'rating': rating,\n",
    "                'reviews': reviews\n",
    "            }\n",
    "            products.append(product)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping product: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "url = \"https://www.meesho.com/men-clothing/pl/9om\"\n",
    "df_meesho = scrape_meesho(url)\n",
    "print(df_meesho)\n",
    "df_meesho.to_csv(\"meesho_products.csv\", index=False)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "current_price",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_price",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviews",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b9cc3392-605e-43fb-99d5-34589052ef2e",
       "rows": [
        [
         "0",
         "Urbane Ravishing Men Tshirts",
         "300",
         "300",
         "3.8",
         "1794 Reviews"
        ],
        [
         "1",
         "Trendy Retro Men Shirts",
         "172",
         "172",
         "4.1",
         "19091 Reviews"
        ],
        [
         "2",
         "Trendy Glamorous Men Shirts",
         "216",
         "216",
         "3.9",
         "27233 Reviews"
        ],
        [
         "3",
         "Classic Sensational Men Shirts",
         "190",
         "190",
         "4.1",
         "25166 Reviews"
        ],
        [
         "4",
         "Classic Retro Men Shirts",
         "228",
         "228",
         "3.9",
         "504 Reviews"
        ],
        [
         "5",
         "Classy Fashionista Men Shirts",
         "325",
         "325",
         "NA",
         "0 Reviews"
        ],
        [
         "6",
         "STI Men's Shirts",
         "213",
         "213",
         "4.0",
         "63996 Reviews"
        ],
        [
         "7",
         "Classy Fashionista Men Shirts",
         "242",
         "242",
         "4.5",
         "51 Reviews"
        ],
        [
         "8",
         "Trendy Fabulous Men Shirts",
         "295",
         "295",
         "3.8",
         "9843 Reviews"
        ],
        [
         "9",
         "Trendy Fashionista Men Shirts",
         "459",
         "459",
         "NA",
         "0 Reviews"
        ],
        [
         "10",
         "Fancy Fashionista Men Shorts",
         "347",
         "347",
         "4.0",
         "9725 Reviews"
        ],
        [
         "11",
         "Stylish Glamorous Men Shirts",
         "167",
         "167",
         "3.8",
         "59157 Reviews"
        ],
        [
         "12",
         "Dollar Bigboss Men's Innerwear Vests",
         "148",
         "148",
         "4.3",
         "2201 Reviews"
        ],
        [
         "13",
         "Fancy Unique Men Track Pants",
         "329",
         "329",
         "4.0",
         "15838 Reviews"
        ],
        [
         "14",
         "Dollar Lehar | Force NXT Men's Trunks",
         "430",
         "430",
         "4.1",
         "10054 Reviews"
        ],
        [
         "15",
         "Trendy Fashionable Men Shirts",
         "260",
         "260",
         "3.9",
         "5744 Reviews"
        ],
        [
         "16",
         "Men Polyester Regular Tshirts",
         "92",
         "92",
         "3.8",
         "1418 Reviews"
        ],
        [
         "17",
         "Gespo Men's Tshirts",
         "241",
         "241",
         "3.9",
         "5259 Reviews"
        ],
        [
         "18",
         "Men Polyester Regular Tshirts",
         "276",
         "276",
         "NA",
         "0 Reviews"
        ],
        [
         "19",
         "STI Men's Shirts",
         "205",
         "205",
         "4.0",
         "11263 Reviews"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>current_price</th>\n",
       "      <th>original_price</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urbane Ravishing Men Tshirts</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1794 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trendy Retro Men Shirts</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>4.1</td>\n",
       "      <td>19091 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trendy Glamorous Men Shirts</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>3.9</td>\n",
       "      <td>27233 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classic Sensational Men Shirts</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>4.1</td>\n",
       "      <td>25166 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Classic Retro Men Shirts</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>3.9</td>\n",
       "      <td>504 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Classy Fashionista Men Shirts</td>\n",
       "      <td>325</td>\n",
       "      <td>325</td>\n",
       "      <td>NA</td>\n",
       "      <td>0 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>STI Men's Shirts</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>4.0</td>\n",
       "      <td>63996 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Classy Fashionista Men Shirts</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>4.5</td>\n",
       "      <td>51 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Trendy Fabulous Men Shirts</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>3.8</td>\n",
       "      <td>9843 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trendy Fashionista Men Shirts</td>\n",
       "      <td>459</td>\n",
       "      <td>459</td>\n",
       "      <td>NA</td>\n",
       "      <td>0 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fancy Fashionista Men Shorts</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9725 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Stylish Glamorous Men Shirts</td>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>3.8</td>\n",
       "      <td>59157 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dollar Bigboss Men's Innerwear Vests</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2201 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fancy Unique Men Track Pants</td>\n",
       "      <td>329</td>\n",
       "      <td>329</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15838 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dollar Lehar | Force NXT Men's Trunks</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>4.1</td>\n",
       "      <td>10054 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Trendy Fashionable Men Shirts</td>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5744 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Men Polyester Regular Tshirts</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1418 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Gespo Men's Tshirts</td>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5259 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Men Polyester Regular Tshirts</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>NA</td>\n",
       "      <td>0 Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>STI Men's Shirts</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11263 Reviews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name current_price original_price rating  \\\n",
       "0            Urbane Ravishing Men Tshirts           300            300    3.8   \n",
       "1                 Trendy Retro Men Shirts           172            172    4.1   \n",
       "2             Trendy Glamorous Men Shirts           216            216    3.9   \n",
       "3          Classic Sensational Men Shirts           190            190    4.1   \n",
       "4                Classic Retro Men Shirts           228            228    3.9   \n",
       "5           Classy Fashionista Men Shirts           325            325     NA   \n",
       "6                        STI Men's Shirts           213            213    4.0   \n",
       "7           Classy Fashionista Men Shirts           242            242    4.5   \n",
       "8              Trendy Fabulous Men Shirts           295            295    3.8   \n",
       "9           Trendy Fashionista Men Shirts           459            459     NA   \n",
       "10           Fancy Fashionista Men Shorts           347            347    4.0   \n",
       "11           Stylish Glamorous Men Shirts           167            167    3.8   \n",
       "12   Dollar Bigboss Men's Innerwear Vests           148            148    4.3   \n",
       "13           Fancy Unique Men Track Pants           329            329    4.0   \n",
       "14  Dollar Lehar | Force NXT Men's Trunks           430            430    4.1   \n",
       "15          Trendy Fashionable Men Shirts           260            260    3.9   \n",
       "16          Men Polyester Regular Tshirts            92             92    3.8   \n",
       "17                    Gespo Men's Tshirts           241            241    3.9   \n",
       "18          Men Polyester Regular Tshirts           276            276     NA   \n",
       "19                       STI Men's Shirts           205            205    4.0   \n",
       "\n",
       "          reviews  \n",
       "0    1794 Reviews  \n",
       "1   19091 Reviews  \n",
       "2   27233 Reviews  \n",
       "3   25166 Reviews  \n",
       "4     504 Reviews  \n",
       "5       0 Reviews  \n",
       "6   63996 Reviews  \n",
       "7      51 Reviews  \n",
       "8    9843 Reviews  \n",
       "9       0 Reviews  \n",
       "10   9725 Reviews  \n",
       "11  59157 Reviews  \n",
       "12   2201 Reviews  \n",
       "13  15838 Reviews  \n",
       "14  10054 Reviews  \n",
       "15   5744 Reviews  \n",
       "16   1418 Reviews  \n",
       "17   5259 Reviews  \n",
       "18      0 Reviews  \n",
       "19  11263 Reviews  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meesho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electronics - Flipkart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 23 products. Saved to flipkart_electronics_products.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Selenium with headless mode\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in background\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "def scrape_flipkart(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for page to load\n",
    "    \n",
    "    # Scroll to load all products dynamically\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Parse the fully loaded page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    # Extract product details from Flipkart's HTML structure\n",
    "    for item in soup.find_all('div', class_='Ip5wZF'):  # Product container selector\n",
    "        try:\n",
    "            brand_element = item.find('div', class_='H0KV9w').text.strip() if item.find('div', class_='H0KV9w') else \"Brand not found\"\n",
    "            name_element = item.find('div', class_='ZHvV68').text.strip() if item.find('div', class_='ZHvV68') else \"Name not found\"\n",
    "            price_element = item.find('div', class_='J5MN75').text.strip() if item.find('div', class_='J5MN75') else \"Price not found\"\n",
    "            original_price_element = price_element  # Flipkart does not always show original price separately\n",
    "            product_id_element = item.find('a')['data-tkid'] if item.find('a') and 'data-tkid' in item.find('a').attrs else \"Product ID not found\"\n",
    "            \n",
    "            product = {\n",
    "                'brand': brand_element,\n",
    "                'product_name': name_element,\n",
    "                'product_id': product_id_element,\n",
    "                'discounted_price': price_element.replace('₹', '').strip(),\n",
    "                'original_price': original_price_element.replace('₹', '').strip()\n",
    "            }\n",
    "            products.append(product)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping product: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "url = \"https://www.flipkart.com/flipkart-electronics-new-store\"\n",
    "df_flipkart = scrape_flipkart(url)\n",
    "\n",
    "# Save the scraped data to a CSV file\n",
    "if not df.empty:\n",
    "    df_flipkart.to_csv(\"flipkart_electronics_products.csv\", index=False)\n",
    "    print(f\"Scraped {len(df_flipkart)} products. Saved to flipkart_electronics_products.csv\")\n",
    "else:\n",
    "    print(\"No products found\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "brand",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "product_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "product_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "discounted_price",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_price",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "78683094-4b4f-4af6-95df-1669186fb7cf",
       "rows": [
        [
         "0",
         "realme, Philips & more",
         "Best of Hair Dryers",
         "Product ID not found",
         "From 549",
         "From 549"
        ],
        [
         "1",
         "Philips, Vega, Dyson & more",
         "Best of Hair Curlers",
         "Product ID not found",
         "From 999",
         "From 999"
        ],
        [
         "2",
         "Oral-B, Mi, realme & more",
         "Electric Toothbrushes",
         "Product ID not found",
         "From 249",
         "From 249"
        ],
        [
         "3",
         "Philips, Vega & more",
         "Best Selling Trimmers",
         "Product ID not found",
         "From 629",
         "From 629"
        ],
        [
         "4",
         "Dr. Trust, Dr. Morepen & more",
         "Best of Vaporizers",
         "Product ID not found",
         "From 999",
         "From 999"
        ],
        [
         "5",
         "Braun & Philips",
         "Best of Epilators",
         "Product ID not found",
         "Upto 45% Off",
         "Upto 45% Off"
        ],
        [
         "6",
         "Trimmers, Dryers & more",
         "Philips Styling Range",
         "Product ID not found",
         "Up to 35% Off",
         "Up to 35% Off"
        ],
        [
         "7",
         "Veet, Braun, Philips & more",
         "Best of Women Trimmers",
         "Product ID not found",
         "From 599",
         "From 599"
        ],
        [
         "8",
         "Explore your photography skill",
         "Mobile phone Lens",
         "Product ID not found",
         "Grab it!",
         "Grab it!"
        ],
        [
         "9",
         "For viewing pleasure",
         "Screen Phone Expander",
         "Product ID not found",
         "gRAB IT!",
         "gRAB IT!"
        ],
        [
         "10",
         "Keep Your Essentials Handy!!",
         "Mobile Holders",
         "Product ID not found",
         "Protect your Acc",
         "Protect your Acc"
        ],
        [
         "11",
         "by boAt, Ambrane & More",
         "Micro USB Cables",
         "Product ID not found",
         "For all phones",
         "For all phones"
        ],
        [
         "12",
         "Light up your Selfies!",
         "Mobile Flash",
         "Product ID not found",
         "Light up your picture",
         "Light up your picture"
        ],
        [
         "13",
         "By Philips, Voltaa & more",
         "Selfie Sticks",
         "Product ID not found",
         "Click picture anywhr",
         "Click picture anywhr"
        ],
        [
         "14",
         "Maintain General Hygiene",
         "Mobile Cleaning Kits",
         "Product ID not found",
         "Clean up your device",
         "Clean up your device"
        ],
        [
         "15",
         "By top brands",
         "Mobile Chargers",
         "Product ID not found",
         "From 99",
         "From 99"
        ],
        [
         "16",
         "Top Selling",
         "Webcam",
         "Product ID not found",
         "Shop Now!",
         "Shop Now!"
        ],
        [
         "17",
         "Top Selling",
         "Keyboards",
         "Product ID not found",
         "Shop Now!!",
         "Shop Now!!"
        ],
        [
         "18",
         "At Low Price",
         "USB Gadgets",
         "Product ID not found",
         "Shop Now!",
         "Shop Now!"
        ],
        [
         "19",
         "Logitech, Dell, HP...",
         "Keyboards",
         "Product ID not found",
         "Shop Now!",
         "Shop Now!"
        ],
        [
         "20",
         "At Best Prices",
         "Computer Accessories",
         "Product ID not found",
         "Shop Now!!!",
         "Shop Now!!!"
        ],
        [
         "21",
         "At Best Price",
         "Mouse",
         "Product ID not found",
         "Shop Now!!",
         "Shop Now!!"
        ],
        [
         "22",
         "TP-Link, D-Link...",
         "Routers",
         "Product ID not found",
         "Shop Now!",
         "Shop Now!"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 23
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_id</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>original_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realme, Philips &amp; more</td>\n",
       "      <td>Best of Hair Dryers</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>From 549</td>\n",
       "      <td>From 549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Philips, Vega, Dyson &amp; more</td>\n",
       "      <td>Best of Hair Curlers</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>From 999</td>\n",
       "      <td>From 999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oral-B, Mi, realme &amp; more</td>\n",
       "      <td>Electric Toothbrushes</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>From 249</td>\n",
       "      <td>From 249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Philips, Vega &amp; more</td>\n",
       "      <td>Best Selling Trimmers</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>From 629</td>\n",
       "      <td>From 629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dr. Trust, Dr. Morepen &amp; more</td>\n",
       "      <td>Best of Vaporizers</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>From 999</td>\n",
       "      <td>From 999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Braun &amp; Philips</td>\n",
       "      <td>Best of Epilators</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Upto 45% Off</td>\n",
       "      <td>Upto 45% Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Trimmers, Dryers &amp; more</td>\n",
       "      <td>Philips Styling Range</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Up to 35% Off</td>\n",
       "      <td>Up to 35% Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Veet, Braun, Philips &amp; more</td>\n",
       "      <td>Best of Women Trimmers</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>From 599</td>\n",
       "      <td>From 599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Explore your photography skill</td>\n",
       "      <td>Mobile phone Lens</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Grab it!</td>\n",
       "      <td>Grab it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For viewing pleasure</td>\n",
       "      <td>Screen Phone Expander</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>gRAB IT!</td>\n",
       "      <td>gRAB IT!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Keep Your Essentials Handy!!</td>\n",
       "      <td>Mobile Holders</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Protect your Acc</td>\n",
       "      <td>Protect your Acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>by boAt, Ambrane &amp; More</td>\n",
       "      <td>Micro USB Cables</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>For all phones</td>\n",
       "      <td>For all phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Light up your Selfies!</td>\n",
       "      <td>Mobile Flash</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Light up your picture</td>\n",
       "      <td>Light up your picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>By Philips, Voltaa &amp; more</td>\n",
       "      <td>Selfie Sticks</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Click picture anywhr</td>\n",
       "      <td>Click picture anywhr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Maintain General Hygiene</td>\n",
       "      <td>Mobile Cleaning Kits</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Clean up your device</td>\n",
       "      <td>Clean up your device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>By top brands</td>\n",
       "      <td>Mobile Chargers</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>From 99</td>\n",
       "      <td>From 99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Top Selling</td>\n",
       "      <td>Webcam</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Shop Now!</td>\n",
       "      <td>Shop Now!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Top Selling</td>\n",
       "      <td>Keyboards</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Shop Now!!</td>\n",
       "      <td>Shop Now!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>At Low Price</td>\n",
       "      <td>USB Gadgets</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Shop Now!</td>\n",
       "      <td>Shop Now!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Logitech, Dell, HP...</td>\n",
       "      <td>Keyboards</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Shop Now!</td>\n",
       "      <td>Shop Now!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>At Best Prices</td>\n",
       "      <td>Computer Accessories</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Shop Now!!!</td>\n",
       "      <td>Shop Now!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>At Best Price</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Shop Now!!</td>\n",
       "      <td>Shop Now!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TP-Link, D-Link...</td>\n",
       "      <td>Routers</td>\n",
       "      <td>Product ID not found</td>\n",
       "      <td>Shop Now!</td>\n",
       "      <td>Shop Now!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             brand            product_name  \\\n",
       "0           realme, Philips & more     Best of Hair Dryers   \n",
       "1      Philips, Vega, Dyson & more    Best of Hair Curlers   \n",
       "2        Oral-B, Mi, realme & more   Electric Toothbrushes   \n",
       "3             Philips, Vega & more   Best Selling Trimmers   \n",
       "4    Dr. Trust, Dr. Morepen & more      Best of Vaporizers   \n",
       "5                  Braun & Philips       Best of Epilators   \n",
       "6          Trimmers, Dryers & more   Philips Styling Range   \n",
       "7      Veet, Braun, Philips & more  Best of Women Trimmers   \n",
       "8   Explore your photography skill       Mobile phone Lens   \n",
       "9             For viewing pleasure   Screen Phone Expander   \n",
       "10    Keep Your Essentials Handy!!          Mobile Holders   \n",
       "11         by boAt, Ambrane & More        Micro USB Cables   \n",
       "12          Light up your Selfies!            Mobile Flash   \n",
       "13       By Philips, Voltaa & more           Selfie Sticks   \n",
       "14        Maintain General Hygiene    Mobile Cleaning Kits   \n",
       "15                   By top brands         Mobile Chargers   \n",
       "16                     Top Selling                  Webcam   \n",
       "17                     Top Selling               Keyboards   \n",
       "18                    At Low Price             USB Gadgets   \n",
       "19           Logitech, Dell, HP...               Keyboards   \n",
       "20                  At Best Prices    Computer Accessories   \n",
       "21                   At Best Price                   Mouse   \n",
       "22              TP-Link, D-Link...                 Routers   \n",
       "\n",
       "              product_id       discounted_price         original_price  \n",
       "0   Product ID not found               From 549               From 549  \n",
       "1   Product ID not found               From 999               From 999  \n",
       "2   Product ID not found               From 249               From 249  \n",
       "3   Product ID not found               From 629               From 629  \n",
       "4   Product ID not found               From 999               From 999  \n",
       "5   Product ID not found           Upto 45% Off           Upto 45% Off  \n",
       "6   Product ID not found          Up to 35% Off          Up to 35% Off  \n",
       "7   Product ID not found               From 599               From 599  \n",
       "8   Product ID not found               Grab it!               Grab it!  \n",
       "9   Product ID not found               gRAB IT!               gRAB IT!  \n",
       "10  Product ID not found       Protect your Acc       Protect your Acc  \n",
       "11  Product ID not found         For all phones         For all phones  \n",
       "12  Product ID not found  Light up your picture  Light up your picture  \n",
       "13  Product ID not found   Click picture anywhr   Click picture anywhr  \n",
       "14  Product ID not found   Clean up your device   Clean up your device  \n",
       "15  Product ID not found                From 99                From 99  \n",
       "16  Product ID not found              Shop Now!              Shop Now!  \n",
       "17  Product ID not found             Shop Now!!             Shop Now!!  \n",
       "18  Product ID not found              Shop Now!              Shop Now!  \n",
       "19  Product ID not found              Shop Now!              Shop Now!  \n",
       "20  Product ID not found            Shop Now!!!            Shop Now!!!  \n",
       "21  Product ID not found             Shop Now!!             Shop Now!!  \n",
       "22  Product ID not found              Shop Now!              Shop Now!  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flipkart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flipkart - Mobiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 18 products. Saved to flipkart_mobile_products.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "def scrape_specifications(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        specifications = {}\n",
    "        \n",
    "        # Find the specifications table\n",
    "        spec_div = soup.find('div', class_='_1OjC5I')  # Main container\n",
    "        \n",
    "        if spec_div:\n",
    "            # Find all the 'General' specification tables\n",
    "            tables = spec_div.find_all('div', class_='GNDEQ-')\n",
    "            \n",
    "            for table in tables:\n",
    "                # Find the table headers\n",
    "                header = table.find('div', class_='_4BJ2V+')\n",
    "                if header:\n",
    "                    category = header.text.strip()\n",
    "                    specifications[category] = {}\n",
    "                    \n",
    "                    # Extract table rows\n",
    "                    table_element = table.find('table', class_='_0ZhAN9')\n",
    "                    if table_element:\n",
    "                        rows = table_element.find_all('tr', class_='WJdYP6 row')\n",
    "                        for row in rows:\n",
    "                            # Extract the key and value from each row\n",
    "                            cols = row.find_all('td')\n",
    "                            if len(cols) == 2:\n",
    "                                key = cols[0].text.strip()\n",
    "                                value = cols[1].text.strip()\n",
    "                                specifications[category][key] = value\n",
    "        return specifications\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping specifications: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def scrape_flipkart(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Scroll to load all products dynamically\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Parse the fully loaded page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    # Extract product details from Flipkart's HTML structure\n",
    "    for item in soup.find_all('div', class_='Ip5wZF'):\n",
    "        try:\n",
    "            # Extract product name and link\n",
    "            product_link = item.find('a', class_='VJA3rP')\n",
    "            product_name = product_link['title'] if product_link and 'title' in product_link.attrs else \"Name not found\"\n",
    "\n",
    "            # Get the product URL\n",
    "            product_url = \"https://www.flipkart.com\" + product_link['href'] if product_link and 'href' in product_link.attrs else None\n",
    "\n",
    "            # Extract price details\n",
    "            price_element = item.find('div', class_='DMMoT0')\n",
    "            discounted_price = price_element.find('div', class_='Nx9bqj').text.strip() if price_element and price_element.find('div', class_='Nx9bqj') else \"Price not found\"\n",
    "            original_price = price_element.find('div', class_='yRaY8j').text.strip() if price_element and price_element.find('div', class_='yRaY8j') else discounted_price\n",
    "\n",
    "            # Extract discount percentage\n",
    "            discount_percentage = price_element.find('span').text.strip() if price_element and price_element.find('span') else \"No discount\"\n",
    "\n",
    "            # Extract ratings and reviews\n",
    "            ratings_element = item.find('span', class_='Y1HWO0')\n",
    "            rating = ratings_element.find('div', class_='XQDdHH').text.strip() if ratings_element and ratings_element.find('div', class_='XQDdHH') else \"No rating\"\n",
    "            reviews_count = item.find('span', class_='Wphh3N').text.strip() if item.find('span', class_='Wphh3N') else \"No reviews\"\n",
    "\n",
    "            # Get specifications dictionary\n",
    "            specifications = {}\n",
    "            if product_url:\n",
    "                specifications = scrape_specifications(product_url)\n",
    "            \n",
    "            # Model Number\n",
    "            model_number = specifications.get('General', {}).get('Model Number', \"Model Number not available\")\n",
    "            brand = specifications.get('General', {}).get('Model Name', \"Brand not available\")\n",
    "            \n",
    "            product = {\n",
    "                'brand': brand,\n",
    "                'product_name': product_name,\n",
    "                'product_id': model_number,\n",
    "                'rating': rating,\n",
    "                'reviews_count': reviews_count,\n",
    "                'discounted_price': discounted_price.replace('₹', '').strip(),\n",
    "                'original_price': original_price.replace('₹', '').strip(),\n",
    "                'discount_percentage': discount_percentage.replace('% off', '').strip()\n",
    "            }\n",
    "            products.append(product)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping product: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "# Main execution\n",
    "url = \"https://www.flipkart.com/mobile-phones-store?otracker=nmenu_sub_Electronics_0_Mobiles\"\n",
    "df = scrape_flipkart(url)\n",
    "\n",
    "# Save the scraped data to a CSV file\n",
    "if not df.empty:\n",
    "    df.to_csv(\"flip_mobile_products.csv\", index=False)\n",
    "    print(f\"Scraped {len(df)} products. Saved to flipkart_mobile_products.csv\")\n",
    "else:\n",
    "    print(\"No products found\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon - Air Conditioners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Panasonic 1.5 Ton 5 Star Wi-Fi Inverter Smart Split AC (India's 1st Matter Enabled RAC, Copper Condenser, 7in1 Convertible, True AI, 4 Way Swing, PM 0.1 Filter, CS/CU-NU18ZKY5W, White)\n",
      "Panasonic 1.5 Ton 5 Star Wi-Fi Inverter Smart Split AC (India's 1st Matter Enabled RAC, Copper Condenser, 7in1 Convertible, True AI, 4 Way Swing, PM 0.1 Filter, CS/CU-NU18ZKY5W, White)\n",
      "LG 1.5 Ton 3 Star DUAL Inverter Split AC (Copper, AI Convertible 6-in-1, VIRAAT Mode, Diet Mode+, Faster Cooling & Energy Saving, HD Filter with Anti-Virus Protection, US-Q18JNXE, White)\n",
      "Voltas 1.5 ton 3 Star, Inverter Split AC (Copper, 4-in-1 Adjustable Mode, Anti-dust Filter, 183V Vectra CAW, White)\n",
      "Daikin 0.8 Ton 3 Star, Fixed Speed Split AC (Copper, PM 2.5 Filter, 2022 Model, FTL28U, White)\n",
      "Voltas 1 ton 3 Star Inverter Split AC (Copper, 4-in-1 Adjustable Mode, Anti-dust Filter, 123V Vectra CAE, White)\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Carrier 1.5 Ton 3 Star Wi-Fi Smart Flexicool Inverter Split AC (Copper, Convertible 6-in-1 Cooling,Smart Energy Display, HD & PM 2.5 Filter, ESTER EDGE FXi (Wi-Fi), CAI18EE3R35W0,White)\n",
      "Whirlpool 1.5 Ton 5 Star, Magicool Inverter Split AC (MAGICOOL 15T 5S INV CNV S5K2PP0, Copper, Convertible 4-in-1 Cooling Mode, HD Filter White)\n",
      "Cruise 1 Ton 3 Star Inverter Split AC with 7-Stage Air Filtration (100% Copper, Convertible 4-in-1, PM 2.5 Filter, CWCVBL-VQ1W123, White)\n",
      "LG 1.5 Ton 3 Star DUAL Inverter Split AC (Copper, AI Convertible 6-in-1, VIRAAT Mode, Diet Mode+, Faster Cooling & Energy Saving, HD Filter with Anti-Virus Protection, US-Q18JNXE, White)\n",
      "Lloyd 1.5 Ton 3 Star Inverter Split AC (5 in 1 Convertible, Copper, Anti-Viral + PM 2.5 Filter,White with Chrome Deco Strip, GLS18I3FWAGC)\n",
      "Carrier 1 Ton 5 Star Wi-Fi Smart Flexicool Inverter Split AC (Copper, Convertible 6-in-1 Cooling,Smart Energy Display,HD & PM 2.5 Filter, ESTER EDGE FXi (Wi-Fi), CAI12EE5R35W0,White)\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Portable Air Conditioners - Small AC Quiet Personal Air Cooler,USB Powered Mini Desktop Cooling Misting Fan, 1 | 2 | 3 Timer 3 Smart Speeds,360°Adjustment Office, Home, Room, Carved Design (MULTI)\n",
      "Blue Star 1.5 Ton 3 Star, 60 Months Warranty, Wi-Fi Smart Inverter Split AC (Copper, 5 in 1 Convertible Cooling, 4-Way Swing, Turbo Cool, Voice Command, IC318YNUS, White)\n",
      "Cruise 1.5 Ton 3 Star Inverter Split AC with 7-Stage Air Filtration (100% Copper, Convertible 4-in-1, PM 2.5 Filter, CWCVBK-VQ1W173, White)\n",
      "Panasonic 1.5 Ton 3 Star Premium Wi-Fi Inverter Smart Split AC (Matter Enabled, Higher Airflow, Copper Condenser, 7in1 Convertible, True AI, 2-Way, PM 0.1 Filter, CS/CU-SU18AKY3W, White)\n",
      "Panasonic 1.4 Ton 3 Star Inverter Split AC (Powerful Mode, Copper Condenser, 7in1 Convertible, 2-Way, PM 0.1 Filter, CS/CU-SU17AKY3T, White)\n",
      "Daikin 1.5 Ton 3 Star Inverter Split AC (Copper, PM 2.5 Filter, Triple Display, Dew Clean Technology, Coanda Airflow, 2024 Model, MTKL50U, White)\n",
      "Carrier 2 Ton 5 Star Wi-Fi Smart Flexicool Inverter Split AC (Copper, Convertible 6-in-1 Cooling,Smart Energy Display,HD & PM 2.5 Filter, ESTER EDGE FXi (Wi-Fi), CAI24EE5R35W0,White)\n",
      "Godrej 1.5 Ton 3 Star, 5 Years Comprehensive Warranty, 5-In-1 Convertible Cooling, Wood Finish, Inverter Split AC (Copper, 4 Way Air Swing, AC 1.5T SIC 18VTC3 WYB TK, Teak Wood)\n",
      "Hitachi 1 Ton Class 5 Star, 4-Way Swing, ice Clean, Xpandable+, Inverter Split AC (100% Copper, Dust Filter - 5400STXL RAS.G512PCBIBT, White)\n",
      "Godrej 1.5 Ton 3 Star, 5 Years Comprehensive Warranty, 5-In-1 Convertible Cooling, Inverter Split AC (Copper, 2025 Model, Heavy duty cooling at 52 °C, AC1.5T EI 18P3T WZT 3S, White)\n",
      "VICARI-Portable-Air-Conditioner-Small-Ac-Quaite-Personal-Air-cooler-Usb-Power-Mini-Desktop-Cooling-Misting-fan-1/2/3-Timer-With-speed-360-Adjustable-For-Office-Room-Desk-And-Car-(Multi-color)\n",
      "Voltas 1.5 ton 5 Star, Inverter Split AC (Copper, 4-in-1 Adjustable Mode, Anti-dust Filter, 185V Vectra CAR, White)\n",
      "Carrier 1.5 Ton 5 Star Wi-Fi Smart Flexicool Inverter Split AC (Copper, Convertible 6-in-1 Cooling,Smart Energy Display,HD & PM 2.5 Filter, ESTER EDGE FXi (Wi-Fi), CAI19EE5R35W0,White)\n",
      "Godrej 1 Ton 3 Star, 5 Years Comprehensive Warranty, 5-In-1 Convertible Cooling, Inverter Split AC (Copper, I-Sense Technology, AC 1T EI 12TINV3R32-GWA Split, White)\n",
      "Samsung 1 Ton 3 Star Digital Inverter Split AC (Faster & Powerful Cooling even at 58 Degree C, 100% Copper, 5 year comprehensive warranty, 5 Step Convertible, AR50F12D0LHNNA, White)\n",
      "Godrej 1.5 Ton 5 Star, 5 Years Comprehensive Warranty, 5-In-1 Convertible Cooling, Inverter Split AC (Copper, 2025 Model, Heavy duty cooling at 52 °C, AC 1.5T EI 18II5T WZS Split 5S, White)\n",
      "Daikin 1.5 Ton 5 Star Inverter Split AC (Copper, PM 2.5 Filter, MTKM50U, White)\n",
      "Whirlpool 1.0 Ton 3 Star, Magicool Inverter Split AC (MAGICOOL 10T 3S INV CNV S5K1PP0, Copper, Convertible 4-in-1 Cooling Mode, HD Filter White)\n",
      "Carrier 1.5 Ton 3 Star Wi-Fi Smart Flexicool Inverter Split AC (Copper, Convertible 6-in-1 Cooling,Smart Energy Display, HD & PM 2.5 Filter, ESTER EDGE FXi (Wi-Fi), CAI18EE3R35W0,White)\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Name not found\n",
      "Scraped data saved to amazon_air_conditioners.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "\n",
    "def scrape_amazon_product_page(url):\n",
    "    \"\"\"\n",
    "    Scrape the specifications from an Amazon product page.\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "def scrape_amazon(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scroll to load products\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    search_results = soup.find_all(\"div\",class_=\"sg-col-inner\")\n",
    "    # print(search_results)\n",
    "\n",
    "    for product_div in search_results:\n",
    "        try:\n",
    "           \n",
    "            title_element = product_div.find('a', class_='a-link-normal s-line-clamp-2 s-link-style a-text-normal')\n",
    "            # print(title_element)\n",
    "            product_link = product_div.find('a', class_='a-link-normal s-line-clamp-2 s-link-style a-text-normal')  # Adjust class if needed\n",
    "            product_name = product_link.find('span').text.strip() if product_link and product_link.find(\n",
    "                'span') else \"Name not found\"\n",
    "            print(product_name)\n",
    "            # product_name = title_element.text.strip() if title_element else \"Product Name not found\"\n",
    "            product_url = \"https://www.amazon.in\" + title_element['href'] if title_element and 'href' in title_element.attrs else None\n",
    "\n",
    "            rating_element = product_div.find('i', class_='a-icon a-icon-star-small')\n",
    "            product_rating = rating_element.text.strip() if rating_element else \"Rating not found\"\n",
    "            num_ratings_element = product_div.find('span', class_='a-size-base s-underline-text')\n",
    "            num_ratings = num_ratings_element.text.strip() if num_ratings_element else \"Number of ratings not found\"\n",
    "\n",
    "            price_whole_element = product_div.find(\"span\", class_=\"a-price-whole\")\n",
    "            discounted_price = price_whole_element.text.strip() if price_whole_element else \"Price not found\"\n",
    "            price_fraction_element = product_div.find(\"span\", class_=\"a-price-fraction\")\n",
    "            discounted_price += price_fraction_element.text.strip() if price_fraction_element else \"\"\n",
    "\n",
    "            original_price_element = product_div.find(\"span\", class_=\"a-offscreen\")\n",
    "            original_price = original_price_element.text.strip() if original_price_element else discounted_price\n",
    "\n",
    "            discount_percentage = product_div.find(\"span\", class_=\"a-color-secondary\")\n",
    "            discount_percentage = discount_percentage.text.strip() if discount_percentage else \"No discount\"\n",
    "\n",
    "            model_number = re.search(r\"([A-Z0-9\\-_]+)$\", product_name)\n",
    "            model_number = model_number.group(1) if model_number else \"Model Number not found\"\n",
    "            \n",
    "            brand = product_name.split(' ')[0] if product_name else \"Brand not found\"\n",
    "\n",
    "          \n",
    "            product = {\n",
    "                \"brand\": brand,\n",
    "                \"product_name\": product_name,\n",
    "                \"product_url\": product_url,\n",
    "                \"discounted_price\": discounted_price,\n",
    "                \"original_price\": original_price,\n",
    "                \"product_rating\": product_rating,\n",
    "                \"num_ratings\": num_ratings,\n",
    "                \"discount_percentage\": discount_percentage,\n",
    "                \"model_number\": model_number\n",
    "            }\n",
    "\n",
    "            products.append(product)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing a product: {e}\")\n",
    "\n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "\n",
    "# Main\n",
    "url = \"https://www.amazon.in/s?k=air+conditioners&i=appliances&crid=3GQGSY1MZ407T&sprefix=air+conditioners%2Cappliances%2C216&ref=nb_sb_noss_2\"\n",
    "df_amazon = scrape_amazon(url)\n",
    "df_cleaned = df_amazon.dropna(axis=1)\n",
    "\n",
    "# Export to CSV\n",
    "if not df_amazon.empty:\n",
    "    df_cleaned.to_csv(\"amazon_air_conditioners.csv\", index=False)\n",
    "    print(\"Scraped data saved to amazon_air_conditioners.csv\")\n",
    "else:\n",
    "    print(\"No products were found or an error occurred.\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping - PriceHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMSUNG 1 Ton 3 Star Split Inverter AC - White\n",
      "Error extracting data from tooltip.\n",
      "No price history data found.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def scrape_price_history(url):\n",
    "    \"\"\"\n",
    "    Scrape price history from the given Price History App product page URL.\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract product name\n",
    "    try:\n",
    "        product_name = soup.find('h3', class_='text-xl font-semibold mt-6 flex flex-row').find('span', class_='line-clamp-1 pl-2 text-gray-400').text.strip()\n",
    "        print(product_name)\n",
    "    except AttributeError:\n",
    "        print(\"Product name not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Locate the chart container for price history data\n",
    "    chart_container = soup.find('div', id='apexchartspriceHistory')\n",
    "    if not chart_container:\n",
    "        print(\"Price history chart not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract price history data points from tooltips\n",
    "    price_data = []\n",
    "    tooltips = chart_container.find_all('div', class_='apexcharts-tooltip apexcharts-theme-light')\n",
    "    for tooltip in tooltips:\n",
    "        try:\n",
    "            date = tooltip.find('div', class_='apexcharts-tooltip-title').text.strip()  # Extract date\n",
    "            price_text = tooltip.find('div', class_='apexcharts-tooltip-series-group apexcharts-active')\n",
    "            price = price_text.find('span', class_='apexcharts-tooltip-text-y-value').text.strip()  # Extract price\n",
    "\n",
    "            price_data.append({'Product': product_name, 'Date': date, 'Price': price})\n",
    "        except AttributeError:\n",
    "            print(\"Error extracting data from tooltip.\")\n",
    "\n",
    "    return pd.DataFrame(price_data)\n",
    "\n",
    "# URL of the product page on Price History App\n",
    "url = \"https://pricehistoryapp.com/product/samsung-1-ton-3-star-split-inverter-ac-white-953e\"\n",
    "\n",
    "# Scrape price history data\n",
    "price_history_df = scrape_price_history(url)\n",
    "\n",
    "# Save the data to a CSV file if available\n",
    "if not price_history_df.empty:\n",
    "    price_history_df.to_csv(\"samsung_ac_price_history.csv\", index=False)\n",
    "    print(f\"Scraped {len(price_history_df)} price points. Saved to samsung_ac_price_history.csv\")\n",
    "else:\n",
    "    print(\"No price history data found.\")\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed: 403 Client Error: Forbidden for url: https://ph.pricetoolkit.com/api/product/history/updateFromSlug\n",
      "No price history data found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_price_history(api_url, payload, headers):\n",
    "    \"\"\"\n",
    "    Scrape price history from the API response.\n",
    "    \n",
    "    Parameters:\n",
    "        api_url (str): The API endpoint URL.\n",
    "        payload (dict): The POST request payload.\n",
    "        headers (dict): The headers for the POST request.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the price history data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make a POST request to the API\n",
    "        response = requests.post(api_url, json=payload, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for HTTP errors\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract price history data\n",
    "        if 'history' in data:\n",
    "            history = data['history']\n",
    "            price_data = [{'Timestamp': timestamp, 'Price': price} for timestamp, price in history.items()]\n",
    "            \n",
    "            # Convert timestamps to readable dates\n",
    "            for item in price_data:\n",
    "                item['Date'] = pd.to_datetime(int(item['Timestamp']), unit='s').strftime('%Y-%m-%d')\n",
    "            \n",
    "            return pd.DataFrame(price_data)\n",
    "        else:\n",
    "            print(\"No price history found in the response.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://ph.pricetoolkit.com/api/product/history/updateFromSlug\"\n",
    "\n",
    "# Payload (replace `pid` with the actual product ID)\n",
    "payload = {\n",
    "    \"pid\": \"ACNH89DZTBHSKHTA\",  # Product ID\n",
    "}\n",
    "\n",
    "# Headers (replace `auth` with the actual authorization token)\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"auth\": \"EFiACaBFbMFqkAk9/JQPoXAhhZYYs2scjL1BfgVQBd0YHFd6gBHoOIT8wahVjr3Y\",\n",
    "}\n",
    "\n",
    "# Scrape price history\n",
    "price_history_df = scrape_price_history(api_url, payload, headers)\n",
    "\n",
    "# Save to CSV if data is available\n",
    "if not price_history_df.empty:\n",
    "    price_history_df.to_csv(\"price_history.csv\", index=False)\n",
    "    print(f\"Scraped {len(price_history_df)} price points. Saved to 'price_history.csv'.\")\n",
    "else:\n",
    "    print(\"No price history data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed: 403 Client Error: Forbidden for url: https://ph.pricetoolkit.com/api/product/history/updateFromSlug\n",
      "No data to save.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_price_history(api_url, payload, headers):\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        session.headers.update(headers)\n",
    "        \n",
    "        response = session.post(api_url, json=payload)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if 'history' in data:\n",
    "            history = data['history']\n",
    "            price_data = [{'Timestamp': ts, 'Price': price} for ts, price in history.items()]\n",
    "            \n",
    "            # Convert timestamps to dates\n",
    "            df = pd.DataFrame(price_data)\n",
    "            df['Date'] = pd.to_datetime(df['Timestamp'], unit='s').dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            return df[['Date', 'Price']]\n",
    "        else:\n",
    "            print(\"No price history found.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# API Configuration\n",
    "api_url = \"https://ph.pricetoolkit.com/api/product/history/updateFromSlug\"\n",
    "payload = {\"pid\": \"ACNH89DZTBHSKHTA\"}  # Replace with your product ID\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"auth\": \"EFiACaBFbMFqkAk9/JQPoXAhhZYYs2scjL1BfgVQBd0YHFd6gBHoOIT8wahVjr3Y\", # Replace with valid token\n",
    "    \"Referer\": \"https://pricehistoryapp.com/\",\n",
    "    \"Origin\": \"https://pricehistoryapp.com\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "# Scrape data\n",
    "price_history_df = scrape_price_history(api_url, payload, headers)\n",
    "\n",
    "# Save to CSV\n",
    "if not price_history_df.empty:\n",
    "    price_history_df.to_csv(\"price_history.csv\", index=False)\n",
    "    print(\"Data saved to price_history.csv\")\n",
    "else:\n",
    "    print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed on attempt 1: 500 Server Error: Internal Server Error for url: https://ph.pricetoolkit.com/api/product/history/updateFromSlug\n",
      "Retrying in 2 seconds...\n",
      "Request failed on attempt 2: 500 Server Error: Internal Server Error for url: https://ph.pricetoolkit.com/api/product/history/updateFromSlug\n",
      "Retrying in 2 seconds...\n",
      "Request failed on attempt 3: 500 Server Error: Internal Server Error for url: https://ph.pricetoolkit.com/api/product/history/updateFromSlug\n",
      "Max retries reached. Aborting.\n",
      "No data to save.\n",
      "\n",
      "Specifications:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_price_history(api_url, payload, headers):\n",
    "    \"\"\"\n",
    "    Scrape price history from the API response with retries.\n",
    "    \n",
    "    Parameters:\n",
    "        api_url (str): The API endpoint URL.\n",
    "        payload (dict): The POST request payload.\n",
    "        headers (dict): The headers for the POST request.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the price history data.\n",
    "    \"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            session = requests.Session()\n",
    "            session.headers.update(headers)\n",
    "            \n",
    "            response = session.post(api_url, json=payload)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if 'history' in data:\n",
    "                history = data['history']\n",
    "                price_data = [{'Timestamp': ts, 'Price': price} for ts, price in history.items()]\n",
    "                \n",
    "                # Convert timestamps to dates\n",
    "                df = pd.DataFrame(price_data)\n",
    "                df['Date'] = pd.to_datetime(df['Timestamp'], unit='s').dt.strftime('%Y-%m-%d')\n",
    "                \n",
    "                return df[['Date', 'Price']]\n",
    "            else:\n",
    "                print(\"No price history found in response.\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed on attempt {attempt + 1}: {e}\")\n",
    "            if e.response is not None and e.response.status_code == 403:\n",
    "                print(\"Forbidden error. Check your authentication token and permissions.\")\n",
    "                return pd.DataFrame()  # No point in retrying if it's a 403\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Aborting.\")\n",
    "                return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame()  # If all retries fail\n",
    "\n",
    "def extract_specifications(html_content):\n",
    "    \"\"\"\n",
    "    Extract specifications from HTML content, handling nested tables.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Initialize a dictionary to store extracted data\n",
    "    specifications = {}\n",
    "\n",
    "    # Locate specification blocks\n",
    "    spec_blocks = soup.find_all('div', class_='GNDEQ-')\n",
    "\n",
    "    for block in spec_blocks:\n",
    "        # Extract category (e.g., General, Display Features)\n",
    "        category_header = block.find('div', class_='_4BJ2V+')\n",
    "        if category_header:\n",
    "            category_name = category_header.text.strip()\n",
    "            specifications[category_name] = {}\n",
    "\n",
    "            # Locate the specifications table within the block\n",
    "            table = block.find('table', class_='_0ZhAN9')\n",
    "            if table:\n",
    "                # Extract each row from the table\n",
    "                rows = table.find_all('tr', class_='WJdYP6 row')\n",
    "                for row in rows:\n",
    "                    # Extract key and value from each row\n",
    "                    cols = row.find_all('td')\n",
    "                    if len(cols) == 2:\n",
    "                        key = cols[0].text.strip()\n",
    "                        value = cols[1].text.strip()\n",
    "                        specifications[category_name][key] = value\n",
    "\n",
    "    return specifications\n",
    "\n",
    "# API Configuration\n",
    "api_url = \"https://ph.pricetoolkit.com/api/product/history/updateFromSlug\"\n",
    "payload = {\"pid\": \"ACNGFSYTSV96QHZJ\"}  # Replace with your product ID\n",
    "\n",
    "# User agents list\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# Select a random user agent\n",
    "user_agent = random.choice(user_agents)\n",
    "\n",
    "# Headers (replace `auth` with the actual authorization token)\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"auth\": \"DIRbT27BSM2O1+/rsa7ur64Zl2XpPwdtTtRqTsSazj1/lFp99pEdxeXda8ovTT2d\",  # Replace with valid token\n",
    "    \"Referer\": \"https://pricehistoryapp.com/\",\n",
    "    \"Origin\": \"https://pricehistoryapp.com\",\n",
    "    \"User-Agent\": user_agent,\n",
    "}\n",
    "\n",
    "# Scrape data\n",
    "price_history_df = scrape_price_history(api_url, payload, headers)\n",
    "\n",
    "# Save to CSV if data is available\n",
    "if not price_history_df.empty:\n",
    "    price_history_df.to_csv(\"price_history.csv\", index=False)\n",
    "    print(\"Data saved to price_history.csv\")\n",
    "else:\n",
    "    print(\"No data to save.\")\n",
    "\n",
    "\n",
    "# Extract specifications\n",
    "# specifications = extract_specifications(html_content)\n",
    "\n",
    "# Print specifications\n",
    "# print(\"\\nSpecifications:\")\n",
    "# for category, specs in specifications.items():\n",
    "#     print(f\"\\nCategory: {category}\")\n",
    "    # for key, value in specs.items():\n",
    "        # print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed on attempt 1: 500 Server Error: Internal Server Error for url: https://ph.pricetoolkit.com/api/product/history/updateFromSlug\n",
      "500 Server Error. This is likely an issue with the API itself.\n",
      "No data to save.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "def scrape_price_history(api_url, payload, headers):\n",
    "    \"\"\"\n",
    "    Scrape price history from the API response with retries.\n",
    "    \n",
    "    Parameters:\n",
    "        api_url (str): The API endpoint URL.\n",
    "        payload (dict): The POST request payload.\n",
    "        headers (dict): The headers for the POST request.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the price history data.\n",
    "    \"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            session = requests.Session()\n",
    "            session.headers.update(headers)\n",
    "            \n",
    "            response = session.post(api_url, json=payload)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if 'history' in data:\n",
    "                history = data['history']\n",
    "                price_data = [{'Timestamp': ts, 'Price': price} for ts, price in history.items()]\n",
    "                \n",
    "                # Convert timestamps to dates\n",
    "                df = pd.DataFrame(price_data)\n",
    "                df['Date'] = pd.to_datetime(df['Timestamp'], unit='s').dt.strftime('%Y-%m-%d')\n",
    "                \n",
    "                return df[['Date', 'Price']]\n",
    "            else:\n",
    "                print(\"No price history found in response (check 'history' key).\")\n",
    "                print(f\"Full response: {data}\")  # Print the full response\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed on attempt {attempt + 1}: {e}\")\n",
    "            if e.response is not None and e.response.status_code == 500:\n",
    "                print(\"500 Server Error. This is likely an issue with the API itself.\")\n",
    "                break  # No point in retrying if the server is having problems\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Aborting.\")\n",
    "                return pd.DataFrame()\n",
    "    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decoding error: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame()  # If all retries fail\n",
    "\n",
    "# API Configuration\n",
    "api_url = \"https://ph.pricetoolkit.com/api/product/history/updateFromSlug\"\n",
    "payload = {\"pid\": \"ACNGFSYTSV96QHZJ\"}  # Replace with your product ID\n",
    "\n",
    "# User agents list\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# Select a random user agent\n",
    "user_agent = random.choice(user_agents)\n",
    "\n",
    "# Headers (replace `auth` with the actual authorization token)\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"auth\": \"DIRbT27BSM2O1+/rsa7ur64Zl2XpPwdtTtRqTsSazj1/lFp99pEdxeXda8ovTT2d\",  # Replace with valid token\n",
    "    \"Referer\": \"https://pricehistoryapp.com/\",\n",
    "    \"Origin\": \"https://pricehistoryapp.com\",\n",
    "    \"User-Agent\": user_agent,\n",
    "}\n",
    "\n",
    "# Scrape data\n",
    "price_history_df = scrape_price_history(api_url, payload, headers)\n",
    "\n",
    "# Save to CSV if data is available\n",
    "if not price_history_df.empty:\n",
    "    price_history_df.to_csv(\"price_history.csv\", index=False)\n",
    "    print(\"Data saved to price_history.csv\")\n",
    "else:\n",
    "    print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed on attempt 1: 500 Server Error: Internal Server Error for url: https://ph.pricetoolkit.com/api/product/history/updateFromSlug\n",
      "500 Server Error. This is likely an issue with the API itself. Stopping retries.\n",
      "No data to save.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "def scrape_price_history(api_url, payload, headers):\n",
    "    \"\"\"\n",
    "    Scrape price history from the API response with retries.\n",
    "    \n",
    "    Parameters:\n",
    "        api_url (str): The API endpoint URL.\n",
    "        payload (dict): The POST request payload.\n",
    "        headers (dict): The headers for the POST request.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the price history data.\n",
    "    \"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            session = requests.Session()\n",
    "            session.headers.update(headers)\n",
    "            \n",
    "            response = session.post(api_url, json=payload)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if 'history' in data:\n",
    "                history = data['history']\n",
    "                price_data = [{'Timestamp': ts, 'Price': price} for ts, price in history.items()]\n",
    "                \n",
    "                # Convert timestamps to dates\n",
    "                df = pd.DataFrame(price_data)\n",
    "                df['Date'] = pd.to_datetime(df['Timestamp'], unit='s').dt.strftime('%Y-%m-%d')\n",
    "                \n",
    "                return df[['Date', 'Price']]\n",
    "            else:\n",
    "                print(\"No price history found in response (check 'history' key).\")\n",
    "                print(f\"Full response: {data}\")  # Print the full response\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed on attempt {attempt + 1}: {e}\")\n",
    "            if e.response is not None and e.response.status_code == 500:\n",
    "                print(\"500 Server Error. This is likely an issue with the API itself. Stopping retries.\")\n",
    "                break  # No point in retrying if the server is having problems\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Aborting.\")\n",
    "                return pd.DataFrame()\n",
    "    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decoding error: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame()  # If all retries fail\n",
    "\n",
    "# API Configuration\n",
    "api_url = \"https://ph.pricetoolkit.com/api/product/history/updateFromSlug\"\n",
    "payload = {\"pid\": \"ACNGFSYTSV96QHZJ\"}  # Replace with your product ID\n",
    "\n",
    "# User agents list\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# Select a random user agent\n",
    "user_agent = random.choice(user_agents)\n",
    "\n",
    "# Headers (replace `auth` with the actual authorization token)\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"auth\": \"WLHD/wdd6p2Zm/LX8IbaCI2pM8qGQsjSCygXOg9T+3fCln2MGnF2Eay5xuzllKyH\",  # Replace with valid token\n",
    "    \"Referer\": \"https://pricehistoryapp.com/\",\n",
    "    \"Origin\": \"https://pricehistoryapp.com\",\n",
    "    \"User-Agent\": user_agent,\n",
    "}\n",
    "\n",
    "# Scrape data\n",
    "price_history_df = scrape_price_history(api_url, payload, headers)\n",
    "\n",
    "# Save to CSV if data is available\n",
    "if not price_history_df.empty:\n",
    "    price_history_df.to_csv(\"price_history.csv\", index=False)\n",
    "    print(\"Data saved to price_history.csv\")\n",
    "else:\n",
    "    print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to fetch price history: Expecting value: line 1 column 1 (char 0)\n",
      "❌ Could not fetch or save data.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_price_history(slug):\n",
    "    url = f\"https://pricehistoryapp.com/product/daikin-1-8-ton-3-star-split-inverter-ac-white-be13\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        if \"history\" not in data:\n",
    "            print(\"❌ 'history' not found in response.\")\n",
    "            return None\n",
    "\n",
    "        history = data[\"history\"]\n",
    "        df = pd.DataFrame({\n",
    "            \"timestamp\": list(history.keys()),\n",
    "            \"price\": list(history.values())\n",
    "        })\n",
    "\n",
    "        df[\"datetime\"] = df[\"timestamp\"].astype(int).apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        df[\"price\"] = df[\"price\"].astype(float)\n",
    "        df = df[[\"datetime\", \"price\"]].sort_values(\"datetime\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to fetch price history: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ✅ Replace with your product slug\n",
    "product_slug = \"daikin-1-8-ton-3-star-split-inverter-ac-white-be13\"\n",
    "\n",
    "df = fetch_price_history(product_slug)\n",
    "\n",
    "if df is not None:\n",
    "    print(df.head())\n",
    "    df.to_csv(\"price_history.csv\", index=False)\n",
    "    print(\"✅ Data saved to 'price_history.csv'\")\n",
    "else:\n",
    "    print(\"❌ Could not fetch or save data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Could not find price history in the page scripts.\n",
      "❌ Could not fetch or save data.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def fetch_price_history_from_web(slug):\n",
    "    url = f\"https://pricehistoryapp.com/product/{slug}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract all script tags and search for the one containing price history\n",
    "        script_tags = soup.find_all(\"script\")\n",
    "\n",
    "        for script in script_tags:\n",
    "            if \"priceHistory\" in script.text:\n",
    "                match = re.search(r'priceHistory\\s*=\\s*(\\[\\{.*?\\}\\]);', script.text, re.DOTALL)\n",
    "                if match:\n",
    "                    price_data_json = match.group(1)\n",
    "                    price_data = json.loads(price_data_json)\n",
    "\n",
    "                    df = pd.DataFrame(price_data)\n",
    "                    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                    df[\"price\"] = df[\"price\"].astype(float)\n",
    "                    df = df.sort_values(\"date\")\n",
    "\n",
    "                    return df\n",
    "\n",
    "        print(\"❌ Could not find price history in the page scripts.\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ✅ Replace with your slug from pricehistoryapp.com\n",
    "product_slug = \"daikin-1-8-ton-3-star-split-inverter-ac-white-be13\"\n",
    "\n",
    "df = fetch_price_history_from_web(product_slug)\n",
    "\n",
    "if df is not None:\n",
    "    print(df.head())\n",
    "    df.to_csv(\"price_history_scraped.csv\", index=False)\n",
    "    print(\"✅ Data saved to 'price_history_scraped.csv'\")\n",
    "else:\n",
    "    print(\"❌ Could not fetch or save data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name: daikin-1-8-ton-3-star-split-inverter-ac-white-be13\n",
      "Current Price: 49990.0\n",
      "Lowest Price: 46999.0\n",
      "Highest Price: 72900.0\n",
      "Average Price: 51237.0\n",
      "\n",
      "Price History (timestamp: price):\n",
      "1663875170: ₹53891.0\n",
      "1665469701: ₹53891.0\n",
      "1665661706: ₹53891.0\n",
      "1668154767: ₹51990.0\n",
      "1668670773: ₹51990.0\n",
      "1671177645: ₹53891.0\n",
      "1671602162: ₹72900.0\n",
      "1671714959: ₹52990.0\n",
      "1671783672: ₹52990.0\n",
      "1672660296: ₹52990.0\n",
      "1672845112: ₹53891.0\n",
      "1673068609: ₹52600.0\n",
      "1673259640: ₹52990.0\n",
      "1673529632: ₹52990.0\n",
      "1673598959: ₹52990.0\n",
      "1673948730: ₹52900.0\n",
      "1674287725: ₹52900.0\n",
      "1675262141: ₹52900.0\n",
      "1675674683: ₹53891.0\n",
      "1675758057: ₹52900.0\n",
      "1675850766: ₹52900.0\n",
      "1675936284: ₹52900.0\n",
      "1676545272: ₹52900.0\n",
      "1676611871: ₹52900.0\n",
      "1676876491: ₹52900.0\n",
      "1676983581: ₹52900.0\n",
      "1677130294: ₹52900.0\n",
      "1677242790: ₹52900.0\n",
      "1677590277: ₹52900.0\n",
      "1677710531: ₹52900.0\n",
      "1677807429: ₹50999.0\n",
      "1677929006: ₹50999.0\n",
      "1677946587: ₹50999.0\n",
      "1678115463: ₹50999.0\n",
      "1678246749: ₹50999.0\n",
      "1678291899: ₹50999.0\n",
      "1678379675: ₹51499.0\n",
      "1678478335: ₹50999.0\n",
      "1678721944: ₹51499.0\n",
      "1679377020: ₹51499.0\n",
      "1679642809: ₹50999.0\n",
      "1680043312: ₹50999.0\n",
      "1680506237: ₹50999.0\n",
      "1680608819: ₹50999.0\n",
      "1680848836: ₹50999.0\n",
      "1681059137: ₹50999.0\n",
      "1681194136: ₹50999.0\n",
      "1681315348: ₹50999.0\n",
      "1681374438: ₹50999.0\n",
      "1681629742: ₹50999.0\n",
      "1681696348: ₹50999.0\n",
      "1681828044: ₹50999.0\n",
      "1681893460: ₹50999.0\n",
      "1681976550: ₹50999.0\n",
      "1682016752: ₹50999.0\n",
      "1682358451: ₹50999.0\n",
      "1682404373: ₹50999.0\n",
      "1682565449: ₹50999.0\n",
      "1684776803: ₹50999.0\n",
      "1685048647: ₹50999.0\n",
      "1685574307: ₹50999.0\n",
      "1686139110: ₹50999.0\n",
      "1686218906: ₹50999.0\n",
      "1686349417: ₹50999.0\n",
      "1686553722: ₹50999.0\n",
      "1686846525: ₹50999.0\n",
      "1686862716: ₹50999.0\n",
      "1686996223: ₹50999.0\n",
      "1687409921: ₹50999.0\n",
      "1687553649: ₹50999.0\n",
      "1688708657: ₹50590.0\n",
      "1689360608: ₹50999.0\n",
      "1689520483: ₹49999.0\n",
      "1689690565: ₹47999.0\n",
      "1690133321: ₹47999.0\n",
      "1691002220: ₹47999.0\n",
      "1691521569: ₹47999.0\n",
      "1691991115: ₹47999.0\n",
      "1693498410: ₹49499.0\n",
      "1693626131: ₹49499.0\n",
      "1693808473: ₹49499.0\n",
      "1693899491: ₹49499.0\n",
      "1693978352: ₹49499.0\n",
      "1694065011: ₹49499.0\n",
      "1694164736: ₹49499.0\n",
      "1694241772: ₹49499.0\n",
      "1694320970: ₹49499.0\n",
      "1694411556: ₹49499.0\n",
      "1694516200: ₹49499.0\n",
      "1694581992: ₹49499.0\n",
      "1694685313: ₹49499.0\n",
      "1694762698: ₹49499.0\n",
      "1695956267: ₹46999.0\n",
      "1695963758: ₹46999.0\n",
      "1696048618: ₹46999.0\n",
      "1696086386: ₹51499.0\n",
      "1696160352: ₹46999.0\n",
      "1697514613: ₹50390.0\n",
      "1700941088: ₹46999.0\n",
      "1705281377: ₹50890.0\n",
      "1707319820: ₹50990.0\n",
      "1707821421: ₹52990.0\n",
      "1708363858: ₹51990.0\n",
      "1710102182: ₹53999.0\n",
      "1713533459: ₹49990.0\n",
      "1714327114: ₹51990.0\n",
      "1714674777: ₹49990.0\n",
      "1717277963: ₹51990.0\n",
      "1718021723: ₹49990.0\n",
      "1719410843: ₹51990.0\n",
      "1719581188: ₹49990.0\n",
      "1720965271: ₹51990.0\n",
      "1721579121: ₹49990.0\n",
      "1726046401: ₹51990.0\n",
      "1727350219: ₹49750.0\n",
      "1727449895: ₹50990.0\n",
      "1727777886: ₹49440.0\n",
      "1727957917: ₹49990.0\n",
      "1728233467: ₹50990.0\n",
      "1730795737: ₹51990.0\n",
      "1737970472: ₹50490.0\n",
      "1739724392: ₹52760.0\n",
      "1740914810: ₹51990.0\n",
      "1743263119: ₹49990.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the API endpoint and slug\n",
    "url = \"https://ph.pricetoolkit.com/api/product/history/updateFromSlug\"\n",
    "slug = \"daikin-1-8-ton-3-star-split-inverter-ac-white-be13\"\n",
    "\n",
    "# Headers (mimicking a browser request)\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    \"Origin\": \"https://pricehistoryapp.com\",\n",
    "    \"Referer\": \"https://pricehistoryapp.com/\",\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"auth\": \"ZiFKt0wZRz6eXva3zsqVNPrm5Nh257zKinBoR+jwC4KGDS2nS3OcqdtvqQpiMkbk\"  # Use the same token from the request\n",
    "}\n",
    "\n",
    "# Data payload\n",
    "data = {\n",
    "    \"slug\": slug\n",
    "}\n",
    "\n",
    "# Send POST request\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "# Check response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Product Name:\", slug)\n",
    "    print(\"Current Price:\", result['price'])\n",
    "    print(\"Lowest Price:\", result['lowest_price'])\n",
    "    print(\"Highest Price:\", result['highest_price'])\n",
    "    print(\"Average Price:\", result['average_price'])\n",
    "    print(\"\\nPrice History (timestamp: price):\")\n",
    "    for timestamp, price in result['history'].items():\n",
    "        print(f\"{timestamp}: ₹{price}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name: daikin-1-8-ton-3-star-split-inverter-ac-white-be13\n",
      "Current Price: 49990.0\n",
      "Lowest Price: 46999.0\n",
      "Highest Price: 72900.0\n",
      "Average Price: 51237.0\n",
      "\n",
      "Price History (Date: Price):\n",
      "23-09-2022: ₹53891.0\n",
      "11-10-2022: ₹53891.0\n",
      "13-10-2022: ₹53891.0\n",
      "11-11-2022: ₹51990.0\n",
      "17-11-2022: ₹51990.0\n",
      "16-12-2022: ₹53891.0\n",
      "21-12-2022: ₹72900.0\n",
      "22-12-2022: ₹52990.0\n",
      "23-12-2022: ₹52990.0\n",
      "02-01-2023: ₹52990.0\n",
      "04-01-2023: ₹53891.0\n",
      "07-01-2023: ₹52600.0\n",
      "09-01-2023: ₹52990.0\n",
      "12-01-2023: ₹52990.0\n",
      "13-01-2023: ₹52990.0\n",
      "17-01-2023: ₹52900.0\n",
      "21-01-2023: ₹52900.0\n",
      "01-02-2023: ₹52900.0\n",
      "06-02-2023: ₹53891.0\n",
      "07-02-2023: ₹52900.0\n",
      "08-02-2023: ₹52900.0\n",
      "09-02-2023: ₹52900.0\n",
      "16-02-2023: ₹52900.0\n",
      "17-02-2023: ₹52900.0\n",
      "20-02-2023: ₹52900.0\n",
      "21-02-2023: ₹52900.0\n",
      "23-02-2023: ₹52900.0\n",
      "24-02-2023: ₹52900.0\n",
      "28-02-2023: ₹52900.0\n",
      "02-03-2023: ₹52900.0\n",
      "03-03-2023: ₹50999.0\n",
      "04-03-2023: ₹50999.0\n",
      "04-03-2023: ₹50999.0\n",
      "06-03-2023: ₹50999.0\n",
      "08-03-2023: ₹50999.0\n",
      "08-03-2023: ₹50999.0\n",
      "09-03-2023: ₹51499.0\n",
      "11-03-2023: ₹50999.0\n",
      "13-03-2023: ₹51499.0\n",
      "21-03-2023: ₹51499.0\n",
      "24-03-2023: ₹50999.0\n",
      "29-03-2023: ₹50999.0\n",
      "03-04-2023: ₹50999.0\n",
      "04-04-2023: ₹50999.0\n",
      "07-04-2023: ₹50999.0\n",
      "09-04-2023: ₹50999.0\n",
      "11-04-2023: ₹50999.0\n",
      "12-04-2023: ₹50999.0\n",
      "13-04-2023: ₹50999.0\n",
      "16-04-2023: ₹50999.0\n",
      "17-04-2023: ₹50999.0\n",
      "18-04-2023: ₹50999.0\n",
      "19-04-2023: ₹50999.0\n",
      "20-04-2023: ₹50999.0\n",
      "21-04-2023: ₹50999.0\n",
      "24-04-2023: ₹50999.0\n",
      "25-04-2023: ₹50999.0\n",
      "27-04-2023: ₹50999.0\n",
      "22-05-2023: ₹50999.0\n",
      "26-05-2023: ₹50999.0\n",
      "01-06-2023: ₹50999.0\n",
      "07-06-2023: ₹50999.0\n",
      "08-06-2023: ₹50999.0\n",
      "10-06-2023: ₹50999.0\n",
      "12-06-2023: ₹50999.0\n",
      "15-06-2023: ₹50999.0\n",
      "16-06-2023: ₹50999.0\n",
      "17-06-2023: ₹50999.0\n",
      "22-06-2023: ₹50999.0\n",
      "24-06-2023: ₹50999.0\n",
      "07-07-2023: ₹50590.0\n",
      "15-07-2023: ₹50999.0\n",
      "16-07-2023: ₹49999.0\n",
      "18-07-2023: ₹47999.0\n",
      "23-07-2023: ₹47999.0\n",
      "03-08-2023: ₹47999.0\n",
      "09-08-2023: ₹47999.0\n",
      "14-08-2023: ₹47999.0\n",
      "31-08-2023: ₹49499.0\n",
      "02-09-2023: ₹49499.0\n",
      "04-09-2023: ₹49499.0\n",
      "05-09-2023: ₹49499.0\n",
      "06-09-2023: ₹49499.0\n",
      "07-09-2023: ₹49499.0\n",
      "08-09-2023: ₹49499.0\n",
      "09-09-2023: ₹49499.0\n",
      "10-09-2023: ₹49499.0\n",
      "11-09-2023: ₹49499.0\n",
      "12-09-2023: ₹49499.0\n",
      "13-09-2023: ₹49499.0\n",
      "14-09-2023: ₹49499.0\n",
      "15-09-2023: ₹49499.0\n",
      "29-09-2023: ₹46999.0\n",
      "29-09-2023: ₹46999.0\n",
      "30-09-2023: ₹46999.0\n",
      "30-09-2023: ₹51499.0\n",
      "01-10-2023: ₹46999.0\n",
      "17-10-2023: ₹50390.0\n",
      "26-11-2023: ₹46999.0\n",
      "15-01-2024: ₹50890.0\n",
      "07-02-2024: ₹50990.0\n",
      "13-02-2024: ₹52990.0\n",
      "19-02-2024: ₹51990.0\n",
      "11-03-2024: ₹53999.0\n",
      "19-04-2024: ₹49990.0\n",
      "28-04-2024: ₹51990.0\n",
      "03-05-2024: ₹49990.0\n",
      "02-06-2024: ₹51990.0\n",
      "10-06-2024: ₹49990.0\n",
      "26-06-2024: ₹51990.0\n",
      "28-06-2024: ₹49990.0\n",
      "14-07-2024: ₹51990.0\n",
      "21-07-2024: ₹49990.0\n",
      "11-09-2024: ₹51990.0\n",
      "26-09-2024: ₹49750.0\n",
      "27-09-2024: ₹50990.0\n",
      "01-10-2024: ₹49440.0\n",
      "03-10-2024: ₹49990.0\n",
      "06-10-2024: ₹50990.0\n",
      "05-11-2024: ₹51990.0\n",
      "27-01-2025: ₹50490.0\n",
      "16-02-2025: ₹52760.0\n",
      "02-03-2025: ₹51990.0\n",
      "29-03-2025: ₹49990.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the API endpoint and slug\n",
    "url = \"https://ph.pricetoolkit.com/api/product/history/updateFromSlug\"\n",
    "slug = \"daikin-1-8-ton-3-star-split-inverter-ac-white-be13\"\n",
    "\n",
    "# Headers (mimicking a browser request)\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    \"Origin\": \"https://pricehistoryapp.com\",\n",
    "    \"Referer\": \"https://pricehistoryapp.com/\",\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"auth\": \"ZiFKt0wZRz6eXva3zsqVNPrm5Nh257zKinBoR+jwC4KGDS2nS3OcqdtvqQpiMkbk\"\n",
    "}\n",
    "\n",
    "# Data payload\n",
    "data = {\n",
    "    \"slug\": slug\n",
    "}\n",
    "\n",
    "# Send POST request\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "# Check response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Product Name:\", slug)\n",
    "    print(\"Current Price:\", result['price'])\n",
    "    print(\"Lowest Price:\", result['lowest_price'])\n",
    "    print(\"Highest Price:\", result['highest_price'])\n",
    "    print(\"Average Price:\", result['average_price'])\n",
    "\n",
    "    print(\"\\nPrice History (Date: Price):\")\n",
    "    for timestamp, price in result['history'].items():\n",
    "        # Convert Unix timestamp to readable format\n",
    "        readable_date = datetime.fromtimestamp(int(timestamp)).strftime('%d-%m-%Y')\n",
    "        print(f\"{readable_date}: ₹{price}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In The Box: 1 Indoor Unit, 1 Outdoor Unit, Remote Control, User Manual, Warranty Card, Standard Installation Kit\n",
      "Brand: Daikin\n",
      "Model Name: ATKL60UV16V/RKL60UV16V\n",
      "Type: Split\n",
      "Capacity in Tons: 1.8 Ton\n",
      "Star Rating: 3 Star BEE Rating\n",
      "BEE Rating Year: 2023\n",
      "Color: White\n",
      "Cooling and Heating: No\n",
      "Cooling Capacity: 6000 W\n",
      "Compressor: Swing Compressor\n",
      "Dehumidification: Yes\n",
      "Remote Control: Yes\n",
      "Refrigerant: R-32\n",
      "Operating Modes: Auto Mode, Dry Mode, Fan Mode, Cool Mode\n",
      "Condenser Coil: Copper\n",
      "Indoor W x H x D: 90.6 cm x 29.8 cm x 23.5 cm\n",
      "Indoor Unit Weight: 11 kg\n",
      "Outdoor W x H x D: 84.5 cm x 59.5 cm x 30 cm\n",
      "Outdoor Unit Weight: 33.5 kg\n",
      "Indoor Temperature Indicator: Yes\n",
      "Turbo Mode: Yes\n",
      "ISEER: 4 W/W\n",
      "Condenser Fin Type: Blue Fin\n",
      "Air Circulation: 593 CFM\n",
      "Auto Air Swing: Yes\n",
      "Air Flow Direction: 4 Way\n",
      "Anti-bacteria Filter: No\n",
      "Dust Filter: Yes\n",
      "Other Filter Features: Titanium Apatite Deodrizing Air Purifying Filter\n",
      "Auto Restart: Yes\n",
      "Child Lock: Yes\n",
      "Quiet Mode: Yes\n",
      "Sleep Mode: Yes\n",
      "Self Diagnosis: Yes\n",
      "Power Requirement: AC 130 - 285 V, 50 Hz\n",
      "Annual Electricity Consumption: 1161.78 Units\n",
      "Operating Current: 8.65 A\n",
      "LED Backlit Buttons: Yes\n",
      "Night Glow Buttons on Remote: Yes\n",
      "Battery Type: 2 AAA Batteries\n",
      "Installation Details: Please check the offer details to know about availability of installation facilities and charges. The standard installation charges need to be paid directly to the service engineer.\n",
      "Standard installation of air-conditioners covers only: \n",
      "1) Drilling of holes into a brick wall for taking out the pipes. \n",
      "2) Fixing a hole sleeve & cap. \n",
      "3) Fixing the indoor and outdoor unit. \n",
      "4) Connecting indoor and outdoor units using the standard Kit provided by the manufacturer (at additional costunless specified otherwise). \n",
      "5) Wrapping the pipe with seasoning tape. \n",
      "Not covered as part of standard Installation charges are: \n",
      "1) Outdoor unit stand - Rs. 750-1000. \n",
      "2) Extra copper wire - Rs. 600-800 per metre. \n",
      "3) Drain pipe extensionif any - Rs. 100 per metre. \n",
      "4) Wiring extension from the meter to the installation site - Rs. 100 per metre. \n",
      "5) Stabilizerif neededis chargeable. \n",
      "6) Plumbing and masonry work. \n",
      "7) Power-point, MCB fitting and any other electrical work. \n",
      "8) Carpentry work. \n",
      "9) Dismantling, shifting of the old AC's masonry - Rs. 1000-1500. \n",
      "10) Core cutting fabrication and electrical.For Further Details Contact Brand at 011-40319300 / 1860 180 3900customerservice@daikinindia.com\n",
      "Technician Visit Details: Authorized Service Engineer will do the followingProvide replacement to the customer in case part / product not repairable (only after replacement request approved)Repair ServicesRepair/change the defective part\n",
      "Uninstallation Details: In case of returnsUninstallPick-up the product.Flipkart's team will visit the customers locationPack\n",
      "Warranty Summary: 1 Year Warranty on Product and 10 Years Warranty on Compressor and 5 Years Warranty on PCB (Printed Circuit Board)\n",
      "Covered in Warranty: Warranty of the Product is Limited to Manufacturing Defects Only\n",
      "Not Covered in Warranty: Parts: Air Filter, Front Grill is Not Covered in the Warranty. The Warranty Does Not Cover Installation or Demonstration. Accessories External to the System. The Product is Not Used According to the Instructions Given in the Instruction Manual. Defects Caused by Improper Use as Determined by the Company Personnel. Modification, Alteration of Any Nature is Made in the Electrical Circuitry or Physical Construction of the Set. Site (Premises Where the Product is Kept) Conditions that do not Confirm to the Recommended Operating Conditions of the Machine. Defects due to Cause Beyond Control Like Lightning, Abnormal Voltage, Acts of God, While in Transit to Service Centers or Purchaser's Residence\n",
      "Warranty Service Type: Technician Visit\n",
      "Disclaimer: The manufacturing date can be earlier than the model year.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "def fetch_specifications(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Retry strategy\n",
    "    retry = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "\n",
    "    try:\n",
    "        response = session.get(product_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        specs = {}\n",
    "        spec_section = soup.find_all(\"table\")\n",
    "        for table in spec_section:\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                columns = row.find_all(\"td\")\n",
    "                if len(columns) == 2:\n",
    "                    key = columns[0].text.strip()\n",
    "                    value = columns[1].text.strip()\n",
    "                    specs[key] = value\n",
    "        return specs if specs else None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"⚠️ Failed to fetch specifications: {e}\")\n",
    "        return None\n",
    "flipkart_product_url = \"https://www.flipkart.com/daikin-1-8-ton-3-star-split-inverter-ac-white/p/itmd7a34167668ae\"\n",
    "specs = fetch_specifications(flipkart_product_url)\n",
    "\n",
    "if specs:\n",
    "    for key, value in specs.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No specifications found or failed to fetch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ API request failed with status code 403\n",
      "✅ Data saved to product_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# ------- Step 1: Fetch Specifications from Flipkart -------\n",
    "\n",
    "def fetch_specifications(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "\n",
    "    try:\n",
    "        response = session.get(product_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        specs = {}\n",
    "        spec_section = soup.find_all(\"table\")\n",
    "        for table in spec_section:\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                columns = row.find_all(\"td\")\n",
    "                if len(columns) == 2:\n",
    "                    key = columns[0].text.strip()\n",
    "                    value = columns[1].text.strip()\n",
    "                    specs[key] = value\n",
    "        return specs if specs else None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"⚠️ Failed to fetch specifications: {e}\")\n",
    "        return None\n",
    "\n",
    "# ------- Step 2: Fetch Price History from API -------\n",
    "\n",
    "def fetch_price_history(slug):\n",
    "    url = \"https://ph.pricetoolkit.com/api/product/history/updateFromSlug\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"Origin\": \"https://pricehistoryapp.com\",\n",
    "        \"Referer\": \"https://pricehistoryapp.com/\",\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"auth\": \"ZiFKt0wZRz6eXva3zsqVNPrm5Nh257zKinBoR+jwC4KGDS2nS3OcqdtvqQpiMkbk\"\n",
    "    }\n",
    "    data = {\"slug\": slug}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=data)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            price_data = {\n",
    "                \"Product Name\": slug,\n",
    "                \"Current Price\": result.get('price'),\n",
    "                \"Lowest Price\": result.get('lowest_price'),\n",
    "                \"Highest Price\": result.get('highest_price'),\n",
    "                \"Average Price\": result.get('average_price'),\n",
    "                \"Price History\": {\n",
    "                    datetime.fromtimestamp(int(ts)).strftime('%d-%m-%Y'): price\n",
    "                    for ts, price in result.get('history', {}).items()\n",
    "                }\n",
    "            }\n",
    "            return price_data\n",
    "        else:\n",
    "            print(f\"⚠️ API request failed with status code {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# ------- Step 3: Combine & Save to CSV -------\n",
    "\n",
    "def save_to_csv(specs, price_data, filename='product_data.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        writer.writerow([\"SECTION\", \"KEY\", \"VALUE\"])\n",
    "\n",
    "        # Write specifications\n",
    "        writer.writerow([\"Specifications\", \"\", \"\"])\n",
    "        if specs:\n",
    "            for key, value in specs.items():\n",
    "                writer.writerow([\"\", key, value])\n",
    "\n",
    "        # Write price details\n",
    "        writer.writerow([\"Price Details\", \"\", \"\"])\n",
    "        if price_data:\n",
    "            for key in [\"Product Name\", \"Current Price\", \"Lowest Price\", \"Highest Price\", \"Average Price\"]:\n",
    "                writer.writerow([\"\", key, price_data.get(key)])\n",
    "\n",
    "            # Write price history\n",
    "            writer.writerow([\"Price History\", \"\", \"\"])\n",
    "            for date, price in price_data.get(\"Price History\", {}).items():\n",
    "                writer.writerow([\"\", date, f\"₹{price}\"])\n",
    "\n",
    "    print(f\"✅ Data saved to {filename}\")\n",
    "\n",
    "\n",
    "# ------- Main Execution -------\n",
    "\n",
    "flipkart_product_url = \"https://www.flipkart.com/daikin-1-8-ton-3-star-split-inverter-ac-white/p/itmd7a34167668ae\"\n",
    "slug = \"daikin-1-8-ton-3-star-split-inverter-ac-white-be13\"\n",
    "\n",
    "specs = fetch_specifications(flipkart_product_url)\n",
    "price_data = fetch_price_history(slug)\n",
    "\n",
    "save_to_csv(specs, price_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged data saved to 'product_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ---------- Step 1: Fetch Flipkart Specifications ----------\n",
    "\n",
    "def fetch_specifications(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "\n",
    "    try:\n",
    "        response = session.get(product_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        specs = {}\n",
    "        spec_section = soup.find_all(\"table\")\n",
    "        for table in spec_section:\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                columns = row.find_all(\"td\")\n",
    "                if len(columns) == 2:\n",
    "                    key = columns[0].text.strip()\n",
    "                    value = columns[1].text.strip()\n",
    "                    specs[key] = value\n",
    "        return specs if specs else None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"⚠️ Failed to fetch specifications: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Step 2: Merge with Already Saved price_history.csv ----------\n",
    "\n",
    "def merge_spec_and_price_history(specs, price_history_csv=\"price_history.csv\", output_csv=\"product_data.csv\"):\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow([\"SECTION\", \"KEY\", \"VALUE\"])\n",
    "\n",
    "        # Write specifications\n",
    "        writer.writerow([\"Specifications\", \"\", \"\"])\n",
    "        if specs:\n",
    "            for key, value in specs.items():\n",
    "                writer.writerow([\"\", key, value])\n",
    "        else:\n",
    "            writer.writerow([\"\", \"No specifications found\", \"\"])\n",
    "\n",
    "        # Merge price_history.csv content\n",
    "        if os.path.exists(price_history_csv):\n",
    "            writer.writerow([\"Price History\", \"\", \"\"])\n",
    "            with open(price_history_csv, mode='r', encoding='utf-8') as in_file:\n",
    "                reader = csv.reader(in_file)\n",
    "                for row in reader:\n",
    "                    if len(row) == 2:\n",
    "                        writer.writerow([\"\", row[0], row[1]])\n",
    "        else:\n",
    "            writer.writerow([\"\", \"No price history file found\", \"\"])\n",
    "\n",
    "    print(f\"✅ Merged data saved to '{output_csv}'\")\n",
    "\n",
    "# ---------- Main Execution ----------\n",
    "\n",
    "flipkart_product_url = \"https://www.flipkart.com/daikin-1-8-ton-3-star-split-inverter-ac-white/p/itmd7a34167668ae\"\n",
    "specs = fetch_specifications(flipkart_product_url)\n",
    "merge_spec_and_price_history(specs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined data saved to 'combined_product_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# Step 1: Product Details (already fetched earlier)\n",
    "slug = \"daikin-1-8-ton-3-star-split-inverter-ac-white-be13\"\n",
    "\n",
    "# Sample price history data (replace with real result['history'] if reused)\n",
    "result = {\n",
    "    \"price\": 42999,\n",
    "    \"lowest_price\": 38990,\n",
    "    \"highest_price\": 45999,\n",
    "    \"average_price\": 41000,\n",
    "    \"history\": {\n",
    "        \"1714396800\": 45999,\n",
    "        \"1717094400\": 38990,\n",
    "        \"1719686400\": 42999\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 2: Fetch Specs from Flipkart\n",
    "def fetch_specifications(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(product_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        specs = {}\n",
    "        for table in soup.find_all(\"table\"):\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                cols = row.find_all(\"td\")\n",
    "                if len(cols) == 2:\n",
    "                    key = cols[0].text.strip()\n",
    "                    value = cols[1].text.strip()\n",
    "                    specs[key] = value\n",
    "        return specs\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Error fetching specs:\", e)\n",
    "        return {}\n",
    "\n",
    "flipkart_url = \"https://www.flipkart.com/daikin-1-8-ton-3-star-split-inverter-ac-white/p/itmd7a34167668ae\"\n",
    "specs = fetch_specifications(flipkart_url)\n",
    "\n",
    "# Step 3: Save to CSV\n",
    "csv_file = \"combined_product_data.csv\"\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write product header\n",
    "    writer.writerow([\"Product Name\", slug])\n",
    "    writer.writerow([\"Current Price\", result['price']])\n",
    "    writer.writerow([\"Lowest Price\", result['lowest_price']])\n",
    "    writer.writerow([\"Highest Price\", result['highest_price']])\n",
    "    writer.writerow([\"Average Price\", result['average_price']])\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write specifications\n",
    "    writer.writerow([\"Specifications\"])\n",
    "    for key, value in specs.items():\n",
    "        writer.writerow([key, value])\n",
    "    writer.writerow([])\n",
    "\n",
    "    # Write price history\n",
    "    writer.writerow([\"Price History\"])\n",
    "    writer.writerow([\"\", \"Date\", \"Price\"])\n",
    "    for timestamp, price in result['history'].items():\n",
    "        readable_date = datetime.fromtimestamp(int(timestamp)).strftime('%d-%m-%Y')\n",
    "        # 👇 This is where the line goes\n",
    "        writer.writerow([\"\", f'\"{readable_date}\"', f'\"{price}\"'])\n",
    "\n",
    "print(f\"✅ Combined data saved to '{csv_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
